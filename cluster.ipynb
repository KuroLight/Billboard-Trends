{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_df_from_json():\n",
    "    cwd = os.getcwd()\n",
    "    frames = []\n",
    "    path = 'lyrics_json_09-18/'\n",
    "    for file in os.listdir(path):\n",
    "        date = file.split('.')[0].split('_')[-1]\n",
    "        with open(path + file, 'r') as f:\n",
    "            df = pd.DataFrame(json.load(f)).dropna()\n",
    "            df['time'] = pd.Timestamp(date)\n",
    "            frames.append(df)\n",
    "    raw_data = pd.concat(frames)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(lyric):\n",
    "    lyric = lyric.lower()\n",
    "    lyric = re.sub(r'\\[.*?\\]', '', lyric)  # remove [*] pattern\n",
    "    lyric = lyric.replace(\"'s\", '')\n",
    "    lyric = lyric.replace(\"'ve\", '')\n",
    "    lyric = lyric.replace(\"'\", '')  # ' must be ignored\n",
    "    lyric = lyric.replace(\"-\", ' ')\n",
    "\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    lyric = lyric.translate(translator)\n",
    "#     tokens = nltk.word_tokenize(lyric)\n",
    "\n",
    "    return lyric #[w for w in tokens if w not in stopwords]\n",
    "\n",
    "def words2sentence(words):\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_df_from_json()\n",
    "billboard = copy.deepcopy(raw_data)\n",
    "# print(billboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# billboard['lyrics'] = billboard['lyrics'].apply(lambda x: preprocess(x))\n",
    "# # billboard['lyrics'] = billboard['lyrics'].apply(lambda x: [w for w in x if w not in stopwords])\n",
    "# # billboard['lyrics'] = billboard['lyrics'].apply(lambda x: words2sentence(x))\n",
    "# print(billboard.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(billboard['lyrics']) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
